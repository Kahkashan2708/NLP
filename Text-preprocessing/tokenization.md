# ğŸ§  Tokenization â€” NLP Interview & Revision Notes

> **Goal:** Quick reference + interview revision notes on Tokenization (classical + modern NLP)

---

## ğŸ—‚ï¸ Table of Contents

1. [ğŸ“˜ Definition](#-1-definition)
2. [ğŸ¯ Why Tokenization Matters](#-2-why-tokenization-matters)
3. [ğŸ“š Key Concepts & Terms](#-3-key-concepts--terms)
4. [âš™ï¸ Common Tokenizer Types](#-4-common-tokenizer-types)
5. [ğŸ§© Important Functions](#-5-important-functions-hugging-face--general)
6. [ğŸ§  Practical Comparisons](#-6-practical-comparisons)
7. [ğŸ’» Example â€” Hugging Face Tokenizer](#-7-example--hugging-face-tokenizer)
8. [ğŸ’¬ Common Interview Q&A](#-8-common-interview-qa)
9. [âš¡ One-Minute Cheat Sheet](#-9-one-minute-cheat-sheet)
10. [ğŸ“‚ References](#-10-references)

---

## ğŸ“˜ 1. Definition

**Tokenization** â†’ Process of splitting raw text into atomic units (â€œtokensâ€) that a model can process.

**Goal:**  
Convert **text â†’ tokens â†’ token IDs (integers)** using a consistent vocabulary mapping.

---

## ğŸ¯ 2. Why Tokenization Matters

- Controls **vocabulary size** and handling of **rare / OOV words**
- Affects **model input length** and compute cost
- Determines **semantic granularity** (words vs. subwords)
- Impacts downstream tasks (classification, generation, QA)

---

## ğŸ“š 3. Key Concepts & Terms

| ğŸ§© Term | ğŸ” Meaning |
|------|----------|
| **Vocabulary** | Mapping token â†” id |
| **OOV (Out-of-Vocab)** | Words not in vocab, handled via `<UNK>` or subword split |
| **Subword Tokenization** | Splits rare words into smaller known units |
| **BPE (Byte-Pair Encoding)** | Merges frequent symbol pairs iteratively |
| **WordPiece** | Like BPE but uses likelihood-based merges (used in BERT) |
| **Unigram** | Probabilistic subword model (used by SentencePiece) |
| **Character Tokenization** | Each character is a token |
| **Normalization** | Lowercasing, Unicode NFKC, accent stripping |
| **Byte-level Tokenization** | Encodes raw bytes (handles any language) |
| **Special Tokens** | `[CLS]`, `[SEP]`, `[PAD]`, `[UNK]`, `[MASK]` |
| **Offsets** | Map tokens â†” character spans (important for NER/QA) |
| **Padding / Truncation** | Adjust sequence lengths |
| **Attention Mask** | `1` for real tokens, `0` for pads |
| **Token Type IDs** | Distinguish sentence A vs B (BERT) |
| **Detokenization** | Convert tokens/ids â†’ text |

---

## âš™ï¸ 4. Common Tokenizer Types

| âš™ï¸ Type | ğŸ“– Description | ğŸ§  Examples |
|------|--------------|-----------|
| **Whitespace / Rule-based** | Splits by spaces/punctuation | Simple preprocessing |
| **Regex / NLTK / spaCy** | Linguistic tokenization | POS tagging, parsing |
| **WordPiece** | Subword merges by likelihood | BERT |
| **BPE** | Subword merges by frequency | GPT, RoBERTa |
| **SentencePiece** | Language-agnostic subword model | T5, ALBERT |
| **Byte-level BPE** | Operates on bytes | GPT-2, GPT-3 |

---

## ğŸ§© 5. Important Functions (Hugging Face & General)

| ğŸ§® Function | ğŸ§° Purpose |
|-----------|----------|
| `tokenize(text)` | Split text â†’ list of tokens |
| `encode(text, add_special_tokens=True)` | Text â†’ token IDs |
| `batch_encode_plus(texts)` | Batch encode with padding & masks |
| `decode(ids, skip_special_tokens=True)` | Token IDs â†’ text |
| `convert_tokens_to_ids(tokens)` | Map tokens â†’ IDs |
| `convert_ids_to_tokens(ids)` | Map IDs â†’ tokens |
| `add_special_tokens(tokens)` | Add `[CLS]`, `[SEP]`, etc. |
| `pad_sequences(sequences, maxlen)` | Pad sequences |
| `get_attention_mask(input_ids)` | Mask: 1 for tokens, 0 for pads |
| `encode_plus(..., return_offsets_mapping=True)` | Get token-char alignment |
| `train_tokenizer(corpus, vocab_size, model_type)` | Train new tokenizer |

---

## ğŸ§  6. Practical Comparisons

| ğŸ” Type | âœ… Pros | âš ï¸ Cons |
|-------|------|------|
| **Word-level** | Intuitive | OOV problem |
| **Subword-level** | Handles rare words, balanced vocab | More complex |
| **Char-level** | No OOV | Very long sequences |
| **BPE vs WordPiece** | BPE = frequency merges; WordPiece = likelihood merges |
| **SentencePiece** | Trains directly on raw text | Slightly slower |

---

## ğŸ’» 7. Example â€” Hugging Face Tokenizer

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

out = tokenizer(
    "Hello world!",
    padding="max_length",
    max_length=8,
    truncation=True
)

print(out)
# Output:
# {'input_ids': [...], 'attention_mask': [...], 'token_type_ids': [...]}

```

ğŸ—£ï¸ **Explain:**

- **`input_ids`** â†’ Token IDs  
- **`attention_mask`** â†’ `1` for real tokens, `0` for pads

| Key | Description |
|-----|--------------|
| `input_ids` | Token IDs â€” numerical representation of tokens |
| `attention_mask` | `1` for real tokens, `0` for padding tokens |


---

## ğŸ’¬ 8. Common Interview Q&A

| ğŸ’¡ Question | ğŸ§­ One-line Answer |
|-------------|-------------------|
| Why subword tokenization? | Reduces OOV, keeps manageable vocab, handles rare words |
| What is an attention mask? | Binary mask to ignore padded tokens in attention |
| How to handle OOV at inference? | Use `<UNK>` or subword split; byte-level if multilingual |
| How to map model output to text? | Use `offset_mapping` for token-char span alignment |
| Difference between BPE and WordPiece? | BPE uses frequency merges; WordPiece uses likelihood merges |
| What is `[CLS]` and `[SEP]` token used for? | `[CLS]`: classification embedding, `[SEP]`: sentence separation |
| Whatâ€™s the tokenizer output format in Hugging Face? | Dict with `input_ids`, `attention_mask`, `token_type_ids` |
| Why SentencePiece doesnâ€™t need pre-tokenization? | It directly learns on raw text (no whitespace split) |

---

## âš¡ 9. One-Minute Cheat Sheet

ğŸš€ **Pipeline:** `text â†’ tokens â†’ IDs â†’ model`  
ğŸ§¾ **Tokenizer Outputs:** `input_ids`, `attention_mask`, (`token_type_ids`)  
ğŸ·ï¸ **Special Tokens:** `[CLS]`, `[SEP]`, `[PAD]`, `[UNK]`, `[MASK]`  
ğŸ§© **Subword Methods:** `BPE`, `WordPiece`, `Unigram`  
âš™ï¸ **Core Functions:** `tokenize`, `encode`, `decode`, `convert_*`, `batch_encode_plus`  
ğŸ“ **Files:** `tokenizer.json`, `vocab.txt`, `merges.txt`  
ğŸ§  **Interview Tip:** Be able to explain â€œhow tokenization affects model performance.â€

---

## ğŸ“‚ 10. References

- ğŸ“˜ [Hugging Face Tokenizers Docs](https://huggingface.co/docs/tokenizers)
- ğŸ“„ [BERT: WordPiece Tokenizer Paper (2018)](https://arxiv.org/abs/1810.04805)
- ğŸ“„ [SentencePiece Paper (Google, 2018)](https://arxiv.org/abs/1808.06226)
- ğŸ“— [Jurafsky & Martin â€” *Speech and Language Processing (3rd Ed.)*](https://web.stanford.edu/~jurafsky/slp3/)
- ğŸ¥ [CampusX NLP Series (YouTube)](https://www.youtube.com/playlist?list=PLKnIA16_Rmvb1RYR-iTA_hzckhdONtSW4)
- ğŸ“˜ [The Illustrated WordPiece & BPE (Blog)](https://huggingface.co/blog/how-tokenizers-work)

---
